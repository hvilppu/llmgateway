# LlmGateway â€” Tunnettuja ongelmia ja pullonkauloja

## Skaalautuuko 1 000 000 dokumentille?

Skenaario: kannassa on 1 000 000 dokumenttia ja kÃ¤yttÃ¤jÃ¤ kysyy
_"MikÃ¤ oli Suomen keskilÃ¤mpÃ¶tila vuonna 2025?"_

### MitÃ¤ tapahtuu

**Vaihe 1 â€” LLM generoi SQL:n**

```sql
SELECT AVG(c.content.lÃ¤mpÃ¶tila) FROM c
WHERE c.content.pvm >= '2025-01-01' AND c.content.pvm <= '2025-12-31'
```

TÃ¤mÃ¤ on hyvÃ¤ kysely â€” aggregaatio, ei `SELECT *`. Mutta silti...

---

### Pullonkaula 1 â€” Cosmos DB cross-partition full scan ðŸ”´ Kriittisin

Nykyinen partitioavain on `/id` (`tools/seed_cosmos.py`). TÃ¤mÃ¤ tarkoittaa ettÃ¤ **jokainen dokumentti on omassa partitiossaan**. 1M dokumentilla kysely menee jokaiseen partitioon erikseen.

```
Kysely â†’ partition 1 â†’ partition 2 â†’ ... â†’ partition 1 000 000
```

- Kustannus: helposti **10 000â€“100 000 RU** per kysely
- Cosmos DB throttlaa `429 Too Many Requests`
- Circuit breaker nappaa 429:t ja avautuu 30 sekunniksi
- KÃ¤yttÃ¤jÃ¤ saa `503 Service Unavailable`

Jos throttlausta ei tule, kysely saattaa kestÃ¤Ã¤ **useita minuutteja**.

---

### Pullonkaula 2 â€” QueryService ei rajoita tuloksia ðŸ”´

TÃ¤mÃ¤ iskee kun LLM generoi ei-aggregoivan kyselyn, esim.:

```sql
SELECT * FROM c WHERE STARTSWITH(c.content.pvm, '2025')
```

`CosmosQueryService` kÃ¤y koko `FeedIterator`-silmukan lÃ¤pi ilman katkaisua. 300 000 dokumentilla tulos on JSON-tiedosto joka on **satoja megatavuja**. Seurauksena:

- Palvelimen muisti loppuu (OOM)
- Tai tulos lisÃ¤tÃ¤Ã¤n messages-listaan â†’ seuraava GPT-4 kutsu saa giganttiset tokenit

â†’ `QueryService.cs`, `ExecuteQueryAsync`: ei rivirajaa eikÃ¤ tuloskokorajoitusta

---

### Pullonkaula 3 â€” Token-rÃ¤jÃ¤hdys messages-listassa ðŸŸ¡

Vaikka `AVG` palauttaisi vain yhden luvun, jos kÃ¤yttÃ¤jÃ¤ kysyy useita asioita perÃ¤kkÃ¤in samassa agenttiloopissa, messages-lista kasvaa kierros kierrokselta. GPT-4:n konteksti-ikkuna ylittyy â†’ `context_length_exceeded` virhe â†’ `500 Internal Server Error`.

â†’ `ChatEndpoints.cs`, `RunAgentLoopAsync`: messages-lista kasvaa kumulatiivisesti

---

### Pullonkaula 4 â€” Ei aikakatkaisua Cosmos DB -kyselylle ðŸŸ¡

`TimeoutMs = 15 000 ms` koskee vain Azure OpenAI -kutsuja. Cosmos DB -kyselyllÃ¤ ei ole omaa timeoutia koodissa. Jos kysely kestÃ¤Ã¤ 2 minuuttia, gateway odottaa â€” ja ASP.NET Coren oletuspyyntÃ¶timeout voi katkaista koko HTTP-yhteyden ennen kuin vastaus ehtii takaisin.

â†’ `QueryService.cs`: `ExecuteQueryAsync` ilman `CancellationToken`-tukea

---

### Yhteenveto

| Pullonkaula | Taso | Seuraus |
|---|---|---|
| Cross-partition full scan | Cosmos DB | 429 throttle â†’ circuit breaker â†’ 503 |
| QueryService ilman rivirajaa | Koodi | OOM tai token-rÃ¤jÃ¤hdys |
| Messages-listan kasvu | Koodi | context_length_exceeded â†’ 500 |
| Ei Cosmos DB -timeoutia | Koodi | Hidas kutsu roikkuu loputtomiin |

---

### Oikeat korjaukset tÃ¤hÃ¤n skaalaan

**1. Vaihda partitioavain** â€” `/content/paikkakunta` jolloin per-kaupunki-kyselyt osuvat yhteen partitioon. "Suomen keskilÃ¤mpÃ¶tila" vaatii silti cross-partition-kyselyn, mutta 10 partitiota vs 1M on eri maailma.

**2. LisÃ¤Ã¤ TOP-rajoitus QueryServiceen** â€” pakota `SELECT TOP 500` jos kyselyssÃ¤ ei ole aggregaatiota, niin tulosjoukko pysyy hallinnassa.

**3. Pre-aggregated data** â€” laske kuukausi-/vuosikeskiarvot etukÃ¤teen erilliseen containeriin. "KeskilÃ¤mpÃ¶tila 2025" osuu yhteen dokumenttiin eikÃ¤ skannaa mitÃ¤Ã¤n.

**4. Azure SQL tai Synapse** â€” jos data on puhtaasti strukturoitua numeerista dataa tÃ¤ssÃ¤ mittakaavassa, relaatiotietokanta indekseillÃ¤ on oikea tyÃ¶kalu. Cosmos DB on optimoitu pistekyselyihin, ei analyyttisiin aggregaatioihin.

---

## Skaalautuuko usealle palvelininstanssille?

### Pullonkaula 5 â€” In-memory circuit breaker ei toimi horisontaalisessa skaalauksessa ðŸ”´

`InMemoryCircuitBreaker` on `singleton` joka elÃ¤Ã¤ **yhden prosessin muistissa**. Azure App Service skaalaa lisÃ¤Ã¤mÃ¤llÃ¤ instansseja â€” jokainen instanssi saa oman, toisistaan tietÃ¤mÃ¤ttÃ¶mÃ¤n circuit breakerin.

Tilanne 10 instanssilla kun Azure OpenAI alkaa failata:

```
Instanssi 1: failure 1/5, 2/5, 3/5, 4/5, 5/5 â†’ breaker OPEN
Instanssi 2: failure 1/5, 2/5, 3/5, 4/5, 5/5 â†’ breaker OPEN
...
Instanssi 10: failure 1/5, 2/5, 3/5, 4/5, 5/5 â†’ breaker OPEN
```

Yhden instanssin pitÃ¤isi avata breaker 5 epÃ¤onnistumisen jÃ¤lkeen â€” mutta 10 instanssilla Azure OpenAI saa **50 epÃ¤onnistunutta kutsua** ennen kuin kaikki breakerit aukeavat. Skaalaaminen pahentaa tilannetta.

**Korjaus:** Hajautettu circuit breaker jaetulla tilalla (Redis). Kaikki instanssit lukevat ja kirjoittavat samaan tilaan.

â†’ `CircuitBreaker.cs`: `InMemoryCircuitBreaker` â€” tila vain paikallisessa muistissa

---

### Pullonkaula 6 â€” Ei vastausvÃ¤limuistia, identtiset kyselyt osuvat aina Azure OpenAI:hin ðŸŸ¡

Kysymys `"MikÃ¤ oli keskilÃ¤mpÃ¶tila HelsingissÃ¤ helmikuussa 2025?"` tuottaa aina saman vastauksen â€” data ei muutu. Silti jokainen kÃ¤yttÃ¤jÃ¤ joka kysyy saman asian kÃ¤ynnistÃ¤Ã¤ tÃ¤yden agenttiloo pin:

```
KÃ¤yttÃ¤jÃ¤ A â†’ Azure OpenAI (GPT-4) â†’ Cosmos DB â†’ Azure OpenAI â†’ vastaus  ~1 s, ~400 tok
KÃ¤yttÃ¤jÃ¤ B â†’ Azure OpenAI (GPT-4) â†’ Cosmos DB â†’ Azure OpenAI â†’ vastaus  ~1 s, ~400 tok
KÃ¤yttÃ¤jÃ¤ C â†’ Azure OpenAI (GPT-4) â†’ Cosmos DB â†’ Azure OpenAI â†’ vastaus  ~1 s, ~400 tok
```

1 000 identtistÃ¤ kyselyÃ¤ = 1 000 Azure OpenAI -kutsua + 1 000 Cosmos DB -kyselyÃ¤. VÃ¤limuistilla se olisi 1 + 999 vÃ¤limuistiosumaa.

**Korjaus:** `IMemoryCache` tai `IDistributedCache` (Redis) avaimella joka on hash(policy + message). TTL esim. 5 minuuttia â€” datan muuttumattomuuden mukaan.

---

### Pullonkaula 7 â€” Cosmos DB:n oletusindeksointipolitiikka on liian laaja ðŸ”´

Cosmos DB indeksoi oletuksena **jokaisen kentÃ¤n jokaisesta dokumentista**. NykyisessÃ¤ schemassa indeksoidaan `id`, `content.paikkakunta`, `content.pvm`, `content.lampotila` ja `embedding` (1 536 floattia).

1M dokumentilla embedding-kentÃ¤n indeksointi on erityisen tuhoisaa:
- Embedding on 1 536-ulotteinen float-vektori
- Oletusindeksi yrittÃ¤Ã¤ indeksoida sen range-indeksiksi
- TÃ¤mÃ¤ kasvattaa **indeksin koon** ja **kirjoituskustannuksen** moninkertaiseksi

**Konkreettinen seuraus kirjoituksessa:**
Yhden dokumentin lisÃ¤Ã¤minen (`upsert`) maksaa normaalisti ~10 RU. EmbeddingillÃ¤ oletusindeksillÃ¤ se voi nousta **100â€“500 RU:hun** per dokumentti. 1M dokumentin seed = 100Mâ€“500M RU.

**Korjaus:** MÃ¤Ã¤ritÃ¤ eksplisiittinen indeksointipolitiikka joka:
- SisÃ¤llyttÃ¤Ã¤ vain `content.paikkakunta`, `content.pvm`, `content.lampotila`
- Sulkee pois `embedding` range-indeksistÃ¤ (vektori-indeksi on erillinen)

```json
{
  "indexingPolicy": {
    "includedPaths": [
      { "path": "/content/paikkakunta/?" },
      { "path": "/content/pvm/?" },
      { "path": "/content/lampotila/?" }
    ],
    "excludedPaths": [
      { "path": "/embedding/*" },
      { "path": "/*" }
    ]
  }
}
```

â†’ `infra/main.bicep`: Cosmos DB container -mÃ¤Ã¤rittelyssÃ¤ ei eksplisiittistÃ¤ `indexingPolicy`-osioita
